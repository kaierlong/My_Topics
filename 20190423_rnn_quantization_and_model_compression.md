# RNN quantization and model compression
---
> Faster and Smaller

#### 20160824
[Recurrent Neural Networks With Limited Numerical Precision]()

#### 20161130
[Effective Quantization Methods for Recurrent Neural Networks]()

#### 20161201
[ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA]()

#### 20170706
[An Embedded Deep Learning based Word Prediction]()

#### 20171020
[Low Precision RNNs: Quantizing RNNs Without Losing Accuracy]()

#### 20180207
[Effective Quantization Approaches for Recurrent Neural Networks]()

#### 20180215
[Alternating Multi-bit Quantization for Recurrent Neural Networks]()

[DeltaRNN: A Power-efficient Recurrent Neural Network Accelerator]()

#### 20180227

#### 20180601
[Binarized LSTM Language Model]()

#### 20180604
[Dynamically Hierarchy Revolution: DirNet for Compressing Recurrent Neural
Network on Mobile Devices]()

#### 20180926
[Adaptive Pruning of Neural Language Models for Mobile Devices]()

#### 20181022
[Fast Training and Model Compression of Gated RNNs via Singular Value Decomposition]()

#### 2018 USENIX ATC
[DeepCPU: Serving RNN-based Deep Learning Models 10x Faster](https://www.usenix.org/conference/atc18/presentation/zhang-minjia)

#### 20190325
[GRNN: Low-Latency and Scalable RNN Inference on GPUs]()

#### not

Hardware-Oriented Compression of Long Short-Term Memory for Efficient Inference

A Contextual Discretization framework for compressing Recurrent Neural Networks